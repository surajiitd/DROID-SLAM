{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../droid_slam')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import lietorch\n",
    "import cv2\n",
    "import os\n",
    "import glob \n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from droid import Droid\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evo\n",
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.tools import file_interface\n",
    "from evo.core import sync\n",
    "import evo.main_ape as main_ape\n",
    "from evo.core.metrics import PoseRelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def show_image(image):\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    cv2.imshow('image', image / 255.0)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "def print_minmax(arr,desc):\n",
    "    \"\"\"visualize depths and uncertainty of any method\"\"\"\n",
    "    \n",
    "    print(\"*\" * 60)\n",
    "    print(\"***{}***  :\".format(desc))\n",
    "    print(\"arr.shape = {}\".format(arr.shape))\n",
    "    print(\"type(arr[0,0] = {}\".format(type(arr[0,0])))\n",
    "    print(\"np.min = {}\".format(np.min(arr)))\n",
    "    print(\"np.max = {}\".format(np.max(arr)))\n",
    "    print(\"np.mean = {}\".format(np.mean(arr)))\n",
    "    print(\"np.median = {}\".format(np.median(arr)))\n",
    "    #print(\"arr[200:220,200:220] = \\n\",arr[200:220,200:220])\n",
    "    print(\"arr[0:10,0:10] = \\n\",arr[0:10,0:10])\n",
    "    print(\"*\" * 60 + \"\\n\")\n",
    "\n",
    "def image_stream(datapath, use_depth=False, stride=1, use_pred_depth = False, pixelformer_depth_folder = 'pixelformer_depth'):\n",
    "    \"\"\" image generator \"\"\"\n",
    "\n",
    "    fx, fy, cx, cy = np.loadtxt(os.path.join(datapath, 'calibration.txt')).tolist()\n",
    "    image_list = sorted(glob.glob(os.path.join(datapath, 'rgb', '*.png')))[::stride]\n",
    "    if use_pred_depth:\n",
    "        depth_list = sorted(glob.glob(os.path.join(datapath, pixelformer_depth_folder, '*.png')))[::stride]\n",
    "    else:    \n",
    "        depth_list = sorted(glob.glob(os.path.join(datapath, 'depth', '*.png')))[::stride]\n",
    "    print(len(image_list), len(depth_list))\n",
    "    for t, (image_file, depth_file) in enumerate(zip(image_list, depth_list)):\n",
    "        #print(\"t = \",t)    \n",
    "        image = cv2.imread(image_file)\n",
    "        \n",
    "        height,width,_= image.shape\n",
    "        image = image[height%32:, width%32:,:]\n",
    "        image = cv2.resize(image, (width, height))\n",
    "\n",
    "        if use_pred_depth:\n",
    "            depth = cv2.imread(depth_file, -1) / 1000.0\n",
    "            # if t==0:\n",
    "            #     print(\"initialising depth with 0\")\n",
    "            #depth = depth * 0.\n",
    "            #print_minmax(depth,\"pred_depth\")\n",
    "            #sys.exit(0)\n",
    "        else:\n",
    "            depth = cv2.imread(depth_file, -1) / 5000.0  # cv2.IMREAD_ANYDEPTH\n",
    "            #print_minmax(depth,\"gt_depth\")\n",
    "            #sys.exit(0)\n",
    "            \n",
    "\n",
    "        h0, w0, _ = image.shape\n",
    "        h1 = int(h0 * np.sqrt((384 * 512) / (h0 * w0)))\n",
    "        w1 = int(w0 * np.sqrt((384 * 512) / (h0 * w0)))\n",
    "\n",
    "        image = cv2.resize(image, (w1, h1))\n",
    "        image = image[:h1-h1%8, :w1-w1%8]\n",
    "        image = torch.as_tensor(image).permute(2, 0, 1)\n",
    "        \n",
    "        depth = torch.as_tensor(depth)\n",
    "        depth = F.interpolate(depth[None,None], (h1, w1)).squeeze()\n",
    "        depth = depth[:h1-h1%8, :w1-w1%8]\n",
    "\n",
    "        intrinsics = torch.as_tensor([fx, fy, cx, cy])\n",
    "        intrinsics[0::2] *= (w1 / w0)\n",
    "        intrinsics[1::2] *= (h1 / h0)\n",
    "\n",
    "        if use_depth:\n",
    "            yield t, image[None], depth, intrinsics\n",
    "\n",
    "        else:\n",
    "            yield t, image[None], intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.datapath = \"../datasets/ETH3D/train/plant_scene_1\"\n",
    "        self.weights = \"../droid.pth\"\n",
    "        self.buffer = 1024\n",
    "        self.image_size = [240,320]\n",
    "        self.disable_vis = True\n",
    "        \n",
    "        self.beta = 0.5\n",
    "        self.filter_thresh = 2.0\n",
    "        self.warmup = 8\n",
    "        self.keyframe_thresh = 3.5\n",
    "        self.frontend_thresh = 16.0\n",
    "        self.frontend_window = 16\n",
    "        self.frontend_radius = 1\n",
    "        self.frontend_nms = 0\n",
    "    \n",
    "        self.stereo = False\n",
    "        self.depth = False\n",
    "    \n",
    "        self.backend_thresh = 22.0\n",
    "        self.backend_radius = 2\n",
    "        self.backend_nms = 3\n",
    "        self.upsample = False\n",
    "        self.reconstruction_path = \"../saved_reconstruction\"\n",
    "        self.without_depth = False\n",
    "        self.pixelformer = False\n",
    "\n",
    "args = Args()\n",
    "torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "stride = 1\n",
    "print(\"Running evaluation on {}\".format(args.datapath))\n",
    "print(vars(args)) # or print(args.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def droid_track(pixelformer_depth_folder = 'pixelformer_depth'):\n",
    "    print(\"params:\",vars(args))\n",
    "    tstamps = []\n",
    "    for (t, image, depth, intrinsics) in tqdm(image_stream(args.datapath, use_depth=True, stride=stride, use_pred_depth = args.pixelformer, pixelformer_depth_folder = pixelformer_depth_folder)):\n",
    "        if not args.disable_vis:\n",
    "            show_image(image[0])\n",
    "    \n",
    "        if t == 0:\n",
    "            args.image_size = [image.shape[2], image.shape[3]]\n",
    "            droid = Droid(args)\n",
    "        if args.without_depth:\n",
    "            \n",
    "            droid.track(t, image, intrinsics=intrinsics)\n",
    "        else:\n",
    "            # print(\"t = \",t)    \n",
    "            droid.track(t, image, depth, intrinsics=intrinsics)\n",
    "    \n",
    "    traj_est = droid.terminate(image_stream(args.datapath, use_depth=False, stride=stride))\n",
    "    return traj_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trajectory(traj_est):\n",
    "    print(\"#\"*20 + \" Results...\")\n",
    "    image_path = os.path.join(args.datapath, 'rgb')\n",
    "    images_list = sorted(glob.glob(os.path.join(image_path, '*.png')))[::stride]\n",
    "    tstamps = [float(x.split('/')[-1][:-4]) for x in images_list]\n",
    "    \n",
    "    traj_est = PoseTrajectory3D(\n",
    "        positions_xyz=traj_est[:,:3],\n",
    "        orientations_quat_wxyz=traj_est[:,3:],\n",
    "        timestamps=np.array(tstamps))\n",
    "    \n",
    "    gt_file = os.path.join(args.datapath, 'groundtruth.txt')\n",
    "    traj_ref = file_interface.read_tum_trajectory_file(gt_file)\n",
    "    \n",
    "    traj_ref, traj_est = sync.associate_trajectories(traj_ref, traj_est)\n",
    "    \n",
    "    result = main_ape.ape(traj_ref, traj_est, est_name='traj', \n",
    "        pose_relation=PoseRelation.translation_part, align=True, correct_scale=True) #originally correct_scale was False\n",
    "    \n",
    "    print(result.stats)\n",
    "    print(result)\n",
    "def save_traj(traj):\n",
    "    os.makedirs(\"saved_traj_estimates\",exist_ok=True)\n",
    "    filename = args.datapath.split(\"/\")[-1] + \"_without_depth-\" + str(args.without_depth) + \"_pixel-\" + str(args.pixelformer)\n",
    "    save_path = os.path.join(\"saved_traj_estimates\",filename)\n",
    "    np.save(save_path, traj)\n",
    "    print(\"trajectory saved as:\", filename+\".npy\")\n",
    "    return save_path+\".npy\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(save_path):\n",
    "    traj_est = np.load(save_path)\n",
    "    translation_est = traj_est[:,:3]\n",
    "    \n",
    "    gt_file = os.path.join(args.datapath, 'groundtruth.txt')\n",
    "    traj_ref = file_interface.read_tum_trajectory_file(gt_file)\n",
    "    translation_gt = traj_ref.positions_xyz\n",
    "\n",
    "    fig,ax = plt.subplots(1,2,figsize=(12,7))\n",
    "    plt.suptitle('ETH-3D Dataset')\n",
    "    ax[0].plot(translation_est[:,0],translation_est[:,1], label = \"Estimated Traj\")\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(translation_gt[:,0],translation_gt[:,1], label = \"GT Traj\")\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/cables_1', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': False, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 1180\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [01:15, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "1180 1180\n",
      "trajectory saved as: cables_1_without_depth-False_pixel-False.npy\n",
      "#################### Results...\n",
      "{'rmse': 0.007509604244555598, 'mean': 0.007082647340240851, 'median': 0.00725747650105077, 'std': 0.0024960493912634498, 'min': 0.000818154093833898, 'max': 0.014809809787479363, 'sse': 0.0664323156618003}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t0.014810\n",
      "      mean\t0.007083\n",
      "    median\t0.007257\n",
      "       min\t0.000818\n",
      "      rmse\t0.007510\n",
      "       sse\t0.066432\n",
      "       std\t0.002496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/cables_1\"\n",
    "args.without_depth = False\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_path = save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traj_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"helo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/sfm_house_loop', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': False, 'pixelformer': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556 556\n",
      "initialising depth with 0\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "556it [00:56,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "556 556\n",
      "#################### Results...\n",
      "{'rmse': 0.9055769436950657, 'mean': 0.6987885443714965, 'median': 0.45451882392397175, 'std': 0.5759897318592245, 'min': 0.10234111085056768, 'max': 2.357951934089896, 'sse': 455.95869812936553}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t2.357952\n",
      "      mean\t0.698789\n",
      "    median\t0.454519\n",
      "       min\t0.102341\n",
      "      rmse\t0.905577\n",
      "       sse\t455.958698\n",
      "       std\t0.575990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/sfm_house_loop\"\n",
    "args.without_depth = False\n",
    "args.pixelformer = True\n",
    "\n",
    "traj_est = droid_track(pixelformer_depth_folder = 'pixelformer_depth')\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/sfm_house_loop', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [344, 560], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556 556\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "556it [00:58,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "556 556\n",
      "#################### Results...\n",
      "{'rmse': 1.8065393121814914, 'mean': 1.3517021817749861, 'median': 1.2135431081401358, 'std': 1.1985347296769997, 'min': 0.07059018017548883, 'max': 4.525181218202944, 'sse': 1814.55286327019}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t4.525181\n",
      "      mean\t1.351702\n",
      "    median\t1.213543\n",
      "       min\t0.070590\n",
      "      rmse\t1.806539\n",
      "       sse\t1814.552863\n",
      "       std\t1.198535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/sfm_house_loop\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/table_3_long', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 1180\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [00:56, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "1180 1180\n",
      "trajectory saved as: table_3_long_without_depth-True_pixel-False.npy\n",
      "#################### Results...\n",
      "{'rmse': 0.00715659123219198, 'mean': 0.0063188272657054585, 'median': 0.0053520903358363105, 'std': 0.0033599434594743425, 'min': 0.0005384162464952738, 'max': 0.01658057304588858, 'sse': 0.04261237598981969}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t0.016581\n",
      "      mean\t0.006319\n",
      "    median\t0.005352\n",
      "       min\t0.000538\n",
      "      rmse\t0.007157\n",
      "       sse\t0.042612\n",
      "       std\t0.003360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/table_3_long\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/table_3', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [344, 560], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 1180\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [00:51, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "1180 1180\n",
      "trajectory saved as: table_3_without_depth-True_pixel-False.npy\n",
      "#################### Results...\n",
      "{'rmse': 0.007029612340294209, 'mean': 0.006268652791011161, 'median': 0.005539875413187815, 'std': 0.003181107014950082, 'min': 0.000633829455938819, 'max': 0.015877415527288764, 'sse': 0.04116306956246225}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t0.015877\n",
      "      mean\t0.006269\n",
      "    median\t0.005540\n",
      "       min\t0.000634\n",
      "      rmse\t0.007030\n",
      "       sse\t0.041163\n",
      "       std\t0.003181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/table_3\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/table_3_long_fb', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359 2359\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2359it [01:54, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/table_3_long_fb\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing redundant poses in gt.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "see below code cell : that is generalized code\n",
    "\n",
    "def truncate(num, n):\n",
    "    integer = int(num * (10**n))/(10**n)\n",
    "    return float(integer)\n",
    "dir_path = \"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/table_3_long\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        #read all assoctiations timestamps first (we need only those tstamps)\n",
    "        assoc_lines = assoc.readlines()\n",
    "        required_tstamps = []\n",
    "        for line in assoc_lines:\n",
    "            line_list = line.strip().split(\" \")\n",
    "            #print(truncate(float(line_list[0]),2))\n",
    "            required_tstamps.append(truncate(float(line_list[0]),2))\n",
    "        print(\"len(required_tstamps) =\",len(required_tstamps))\n",
    "        #now make new gt_file with only poses with only required tstamps.\n",
    "        gt_lines = gt.readlines()\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long.txt\"),\"w\") as gt_long:\n",
    "            for gt_line in gt_lines:\n",
    "                if gt_line.startswith(\"#\"):\n",
    "                    continue\n",
    "                gt_line_list = [float(val) for val in gt_line.strip().split(\" \")]\n",
    "                if gt_line_list[0] not in required_tstamps:\n",
    "                    continue\n",
    "                    \n",
    "                #print(gt_line_list)\n",
    "                gt_line_list = [str(val) for val in gt_line_list]\n",
    "                print_line = \" \".join(gt_line_list[1:])\n",
    "                print_line = str(f\"{float(gt_line_list[0]):.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                gt_long.write(print_line)\n",
    "                #print(print_line)\n",
    "                \n",
    "\"\"\"                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2989.01 2989.02 2989.03 2989.04 2989.05 2989.06 2989.07 2989.08 2989.09\n",
      " 2989.1 ]\n",
      "poses_available = 833\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/table_3_long\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        #read all assoctiations timestamps first (we need only those tstamps)\n",
    "            \n",
    "        assoc_lines = assoc.readlines()\n",
    "        \n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        print(gt_tstamps[0:10])\n",
    "        \n",
    "        #now for every image in assoc.txt, find it's corresponding pose and add it to the gt.txt only\n",
    "        # if it is less than max_diff(=.01) apart in timestamps.\n",
    "        max_diff = .01\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long.txt\"),\"w\") as gt_long:\n",
    "            gt_long.write(gt_lines[0]) # adding commented first line\n",
    "            poses_available = 0\n",
    "            for line in assoc_lines:\n",
    "                line_list = line.strip().split(\" \")\n",
    "                #print(truncate(float(line_list[0]),2))\n",
    "                required_tstamp = float(line_list[0])\n",
    "                diff_list = np.abs(gt_tstamps-required_tstamp)\n",
    "                diffs = np.abs(gt_tstamps-required_tstamp)\n",
    "                index = int(np.argmin(diffs))\n",
    "                if diffs[index] <= max_diff:\n",
    "                    \"\"\"add pose for that image,depth pair in gt.txt\"\"\"\n",
    "                    closest_tstamp = gt_tstamps[index]\n",
    "                    #print(f\"closest tstamp to {required_tstamp} in gt is {closest_tstamp}\")\n",
    "                    poses_available += 1\n",
    "                    print_line = \" \".join(gt_dict[closest_tstamp])\n",
    "                    print_line = str(f\"{closest_tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                    gt_long.write(print_line)\n",
    "                    #print(print_line)\n",
    "            print(\"poses_available =\",poses_available)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating long seqeuences manually by FB (forward-backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(integer)\n\u001b[1;32m      5\u001b[0m dir_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/table_3_long_fb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundtruth.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m gt:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massociated.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m assoc:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124;03m\"\"\" making gt.txt\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def truncate(num, n):\n",
    "    integer = int(num * (10**n))/(10**n)\n",
    "    return float(integer)\n",
    "dir_path = \"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/table_3_long_fb\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        \"\"\" making gt.txt\"\"\"\n",
    "        print('\"\"\"\" making gt.txt\"\"\"')\n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        print(\"len(gt_lines) = \",len(gt_lines))\n",
    "        #make gt dict\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(gt_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = gt_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        \n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long_fb.txt\"),\"w\") as gt_long:\n",
    "            for tstamp in gt_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(gt_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                gt_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written gt.txt !!\")\n",
    "\n",
    "        \n",
    "        \"\"\" making association.txt \"\"\"\n",
    "        print('\\n\\n\"\"\" making association.txt \"\"\"')\n",
    "        assoc_lines = assoc.readlines()\n",
    "        print(\"len(assoc_lines) = \",len(assoc_lines))\n",
    "        \n",
    "        #make assoc dict\n",
    "        assoc_dict = {}\n",
    "        for line in assoc_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            assoc_dict[float(line[0])] = line[1:]\n",
    "        assoc_tstamps = np.array(list(assoc_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = assoc_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            \n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind}.png\")\n",
    "\n",
    "            cmd_3 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth/{truncate(float(assoc_dict[corresponding_tstamp][1]),6)}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth/{new_img_ind}.png\")\n",
    "            cmd_4 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth10/{truncate(float(assoc_dict[corresponding_tstamp][1]),6)}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth10/{new_img_ind}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            os.system(cmd_3)\n",
    "            os.system(cmd_4)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        \n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"associated_long_fb.txt\"),\"w\") as assoc_long:\n",
    "            for tstamp in assoc_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(assoc_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                assoc_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written assoc.txt !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating long seqeuences manually by FBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\" making gt.txt\"\"\"\n",
      "len(gt_lines) =  834\n",
      "last_ind =  832\n",
      "verify FB: n = 833, 2(n-1)+1 = 1665 , tstamps now = 1665\n",
      "fb_last_ind =  1664\n",
      "verify FBF: n = 833, 3(n-1)+1 = 2497 , tstamps now = 2497\n",
      "written gt.txt !!\n",
      "\n",
      "\n",
      "\"\"\" making association.txt \"\"\"\n",
      "len(assoc_lines) =  1180\n",
      "last_ind =  1179\n",
      "verify: n = 1180, 2(n-1)+1 = 2359 , tstamps now = 2359\n",
      "fb_last_ind =  2358\n",
      "verify FBF: n = 1180, 3(n-1)+1 = 3538 , tstamps now = 3538\n",
      "written assoc.txt !!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def truncate(num, n):\n",
    "    integer = int(num * (10**n))/(10**n)\n",
    "    return float(integer)\n",
    "dir_path = \"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/table_3_long_fbfb\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        \"\"\" making gt.txt\"\"\"\n",
    "        print('\"\"\"\" making gt.txt\"\"\"')\n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        print(\"len(gt_lines) = \",len(gt_lines))\n",
    "        #make gt dict\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(gt_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = gt_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1 # original no. entries\n",
    "        print(f\"verify FB: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(gt_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= last_ind:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        \n",
    "        print(f\"verify FBF: n = {n}, 3(n-1)+1 = {3*n-2} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long_fbf.txt\"),\"w\") as gt_long:\n",
    "            for tstamp in gt_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(gt_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                gt_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written gt.txt !!\")\n",
    "        \n",
    "        \n",
    "        \"\"\" making association.txt \"\"\"\n",
    "        \n",
    "        print('\\n\\n\"\"\" making association.txt \"\"\"')\n",
    "        assoc_lines = assoc.readlines()\n",
    "        print(\"len(assoc_lines) = \",len(assoc_lines))\n",
    "        \n",
    "        #make assoc dict\n",
    "        assoc_dict = {}\n",
    "        for line in assoc_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            assoc_dict[float(line[0])] = line[1:]\n",
    "        assoc_tstamps = np.array(list(assoc_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = assoc_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            \n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= last_ind:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind}.png\")\n",
    "            #print(cmd_1)\n",
    "            #print(cmd_2)\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        print(f\"verify FBF: n = {n}, 3(n-1)+1 = {3*n-2} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"associated_long_fbf.txt\"),\"w\") as assoc_long:\n",
    "            for tstamp in assoc_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(assoc_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                assoc_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written assoc.txt !!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating long seqeuences manually by FBFB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\" making gt.txt\"\"\"\n",
      "len(gt_lines) =  834\n",
      "last_ind =  832\n",
      "verify FB: n = 833, 2(n-1)+1 = 1665 , tstamps now = 1665\n",
      "fb_last_ind =  1664\n",
      "verify FBF: n = 833, 4(n-1)+1 = 3329 , tstamps now = 3329\n",
      "written gt.txt !!\n",
      "\n",
      "\n",
      "\"\"\" making association.txt \"\"\"\n",
      "len(assoc_lines) =  1180\n",
      "last_ind =  1179\n",
      "verify: n = 1180, 2(n-1)+1 = 2359 , tstamps now = 2359\n",
      "fb_last_ind =  2358\n",
      "verify FBF: n = 1180, 4(n-1)+1 = 4717 , tstamps now = 4717\n",
      "written assoc.txt !!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def truncate(num, n):\n",
    "    integer = int(num * (10**n))/(10**n)\n",
    "    return float(integer)\n",
    "dir_path = \"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/table_3_long_fbfb\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        \"\"\" making gt.txt\"\"\"\n",
    "        print('\"\"\"\" making gt.txt\"\"\"')\n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        print(\"len(gt_lines) = \",len(gt_lines))\n",
    "        #make gt dict\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(gt_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = gt_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1 # original no. entries\n",
    "        print(f\"verify FB: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(gt_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        \n",
    "        print(f\"verify FBF: n = {n}, 4(n-1)+1 = {4*(n-1)+1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long_fbf.txt\"),\"w\") as gt_long:\n",
    "            for tstamp in gt_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(gt_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                gt_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written gt.txt !!\")\n",
    "        \n",
    "        \n",
    "        \"\"\" making association.txt \"\"\"\n",
    "        \n",
    "        print('\\n\\n\"\"\" making association.txt \"\"\"')\n",
    "        assoc_lines = assoc.readlines()\n",
    "        print(\"len(assoc_lines) = \",len(assoc_lines))\n",
    "        \n",
    "        #make assoc dict\n",
    "        assoc_dict = {}\n",
    "        for line in assoc_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            assoc_dict[float(line[0])] = line[1:]\n",
    "        assoc_tstamps = np.array(list(assoc_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = assoc_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            \n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind}.png\")\n",
    "            #print(cmd_1)\n",
    "            #print(cmd_2)\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        print(f\"verify FBF: n = {n}, 4(n-1)+1 = {4*(n-1)+1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"associated_long_fbf.txt\"),\"w\") as assoc_long:\n",
    "            for tstamp in assoc_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(assoc_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                assoc_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written assoc.txt !!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
