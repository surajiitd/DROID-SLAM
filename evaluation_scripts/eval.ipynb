{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../droid_slam')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import lietorch\n",
    "import cv2\n",
    "import os\n",
    "import glob \n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from droid import Droid\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evo\n",
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.tools import file_interface\n",
    "from evo.core import sync\n",
    "import evo.main_ape as main_ape\n",
    "from evo.core.metrics import PoseRelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rough**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 43, 70])\n"
     ]
    }
   ],
   "source": [
    "delta = torch.randn(1, 36, 43, 70, 2)\n",
    "delta2 = torch.randn(1, 36, 43, 70, 2)\n",
    "print((delta-delta2).norm(dim=-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def show_image(image):\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    cv2.imshow('image', image / 255.0)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "def print_minmax(arr,desc):\n",
    "    \"\"\"visualize depths and uncertainty of any method\"\"\"\n",
    "    \n",
    "    print(\"*\" * 60)\n",
    "    print(\"***{}***  :\".format(desc))\n",
    "    print(\"arr.shape = {}\".format(arr.shape))\n",
    "    print(\"type(arr[0,0] = {}\".format(type(arr[0,0])))\n",
    "    print(\"np.min = {}\".format(np.min(arr)))\n",
    "    print(\"np.max = {}\".format(np.max(arr)))\n",
    "    print(\"np.mean = {}\".format(np.mean(arr)))\n",
    "    print(\"np.median = {}\".format(np.median(arr)))\n",
    "    #print(\"arr[200:220,200:220] = \\n\",arr[200:220,200:220])\n",
    "    print(\"arr[0:10,0:10] = \\n\",arr[0:10,0:10])\n",
    "    print(\"*\" * 60 + \"\\n\")\n",
    "\n",
    "def image_stream(datapath, use_depth=False, stride=1, use_pred_depth = False, pixelformer_depth_folder = 'pixelformer_depth'):\n",
    "    \"\"\" image generator \"\"\"\n",
    "\n",
    "    fx, fy, cx, cy = np.loadtxt(os.path.join(datapath, 'calibration.txt')).tolist()\n",
    "    image_list = sorted(glob.glob(os.path.join(datapath, 'rgb', '*.png')))[::stride]\n",
    "    if use_pred_depth:\n",
    "        depth_list = sorted(glob.glob(os.path.join(datapath, pixelformer_depth_folder, '*.png')))[::stride]\n",
    "    else:    \n",
    "        depth_list = sorted(glob.glob(os.path.join(datapath, 'depth', '*.png')))[::stride]\n",
    "    print(len(image_list), len(depth_list))\n",
    "    for t, (image_file, depth_file) in enumerate(zip(image_list, depth_list)):\n",
    "        #print(\"t = \",t)    \n",
    "        image = cv2.imread(image_file)\n",
    "        \n",
    "        height,width,_= image.shape\n",
    "        image = image[height%32:, width%32:,:]\n",
    "        image = cv2.resize(image, (width, height))\n",
    "\n",
    "        if use_pred_depth:\n",
    "            depth = cv2.imread(depth_file, -1) / 1000.0\n",
    "            # if t==0:\n",
    "            #     print(\"initialising depth with 0\")\n",
    "            #depth = depth * 0.\n",
    "            #print_minmax(depth,\"pred_depth\")\n",
    "            #sys.exit(0)\n",
    "        else:\n",
    "            depth = cv2.imread(depth_file, -1) / 5000.0  # cv2.IMREAD_ANYDEPTH\n",
    "            #print_minmax(depth,\"gt_depth\")\n",
    "            #sys.exit(0)\n",
    "            \n",
    "\n",
    "        h0, w0, _ = image.shape\n",
    "        h1 = int(h0 * np.sqrt((384 * 512) / (h0 * w0)))\n",
    "        w1 = int(w0 * np.sqrt((384 * 512) / (h0 * w0)))\n",
    "\n",
    "        image = cv2.resize(image, (w1, h1))\n",
    "        image = image[:h1-h1%8, :w1-w1%8]\n",
    "        image = torch.as_tensor(image).permute(2, 0, 1)\n",
    "        \n",
    "        depth = torch.as_tensor(depth)\n",
    "        depth = F.interpolate(depth[None,None], (h1, w1)).squeeze()\n",
    "        depth = depth[:h1-h1%8, :w1-w1%8]\n",
    "\n",
    "        intrinsics = torch.as_tensor([fx, fy, cx, cy])\n",
    "        intrinsics[0::2] *= (w1 / w0)\n",
    "        intrinsics[1::2] *= (h1 / h0)\n",
    "\n",
    "        if use_depth:\n",
    "            yield t, image[None], depth, intrinsics\n",
    "\n",
    "        else:\n",
    "            yield t, image[None], intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on ../datasets/ETH3D/train/plant_scene_1\n",
      "{'datapath': '../datasets/ETH3D/train/plant_scene_1', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': False, 'pixelformer': False}\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.datapath = \"../datasets/ETH3D/train/plant_scene_1\"\n",
    "        self.weights = \"../droid.pth\"\n",
    "        self.buffer = 1024\n",
    "        self.image_size = [240,320]\n",
    "        self.disable_vis = True\n",
    "        \n",
    "        self.beta = 0.5\n",
    "        self.filter_thresh = 2.0\n",
    "        self.warmup = 8\n",
    "        self.keyframe_thresh = 3.5\n",
    "        self.frontend_thresh = 16.0\n",
    "        self.frontend_window = 16\n",
    "        self.frontend_radius = 1\n",
    "        self.frontend_nms = 0\n",
    "    \n",
    "        self.stereo = False\n",
    "        self.depth = False\n",
    "    \n",
    "        self.backend_thresh = 22.0\n",
    "        self.backend_radius = 2\n",
    "        self.backend_nms = 3\n",
    "        self.upsample = False\n",
    "        self.reconstruction_path = \"../saved_reconstruction\"\n",
    "        self.without_depth = False\n",
    "        self.pixelformer = False\n",
    "\n",
    "args = Args()\n",
    "torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "stride = 1\n",
    "print(\"Running evaluation on {}\".format(args.datapath))\n",
    "print(vars(args)) # or print(args.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def droid_track(pixelformer_depth_folder = 'pixelformer_depth'):\n",
    "    print(\"params:\",vars(args))\n",
    "    tstamps = []\n",
    "    for (t, image, depth, intrinsics) in tqdm(image_stream(args.datapath, use_depth=True, stride=stride, use_pred_depth = args.pixelformer, pixelformer_depth_folder = pixelformer_depth_folder)):\n",
    "        if not args.disable_vis:\n",
    "            show_image(image[0])\n",
    "    \n",
    "        if t == 0:\n",
    "            args.image_size = [image.shape[2], image.shape[3]]\n",
    "            droid = Droid(args)\n",
    "        if args.without_depth:\n",
    "            \n",
    "            droid.track(t, image, intrinsics=intrinsics)\n",
    "        else:\n",
    "            # print(\"t = \",t)    \n",
    "            droid.track(t, image, depth, intrinsics=intrinsics)\n",
    "    \n",
    "    traj_est = droid.terminate(image_stream(args.datapath, use_depth=False, stride=stride))\n",
    "    return traj_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trajectory(traj_est):\n",
    "    print(\"#\"*20 + \" Results...\")\n",
    "    image_path = os.path.join(args.datapath, 'rgb')\n",
    "    images_list = sorted(glob.glob(os.path.join(image_path, '*.png')))[::stride]\n",
    "    tstamps = [float(x.split('/')[-1][:-4]) for x in images_list]\n",
    "    \n",
    "    traj_est = PoseTrajectory3D(\n",
    "        positions_xyz=traj_est[:,:3],\n",
    "        orientations_quat_wxyz=traj_est[:,3:],\n",
    "        timestamps=np.array(tstamps))\n",
    "    \n",
    "    gt_file = os.path.join(args.datapath, 'groundtruth.txt')\n",
    "    traj_ref = file_interface.read_tum_trajectory_file(gt_file)\n",
    "    \n",
    "    traj_ref, traj_est = sync.associate_trajectories(traj_ref, traj_est)\n",
    "    \n",
    "    result = main_ape.ape(traj_ref, traj_est, est_name='traj', \n",
    "        pose_relation=PoseRelation.translation_part, align=True, correct_scale=True) #originally correct_scale was False\n",
    "    \n",
    "    print(result.stats)\n",
    "    print(result)\n",
    "def save_traj(traj):\n",
    "    os.makedirs(\"saved_traj_estimates\",exist_ok=True)\n",
    "    filename = args.datapath.split(\"/\")[-1] + \"_without_depth-\" + str(args.without_depth) + \"_pixel-\" + str(args.pixelformer)\n",
    "    save_path = os.path.join(\"saved_traj_estimates\",filename)\n",
    "    np.save(save_path, traj)\n",
    "    print(\"trajectory saved as:\", filename+\".npy\")\n",
    "    return save_path+\".npy\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(save_path):\n",
    "    traj_est = np.load(save_path)\n",
    "    translation_est = traj_est[:,:3]\n",
    "    \n",
    "    gt_file = os.path.join(args.datapath, 'groundtruth.txt')\n",
    "    traj_ref = file_interface.read_tum_trajectory_file(gt_file)\n",
    "    translation_gt = traj_ref.positions_xyz\n",
    "\n",
    "    fig,ax = plt.subplots(1,2,figsize=(12,7))\n",
    "    plt.suptitle('ETH-3D Dataset')\n",
    "    ax[0].plot(translation_est[:,0],translation_est[:,1], label = \"Estimated Traj\")\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(translation_gt[:,0],translation_gt[:,1], label = \"GT Traj\")\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/cables_1', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': False, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 1180\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [01:15, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "1180 1180\n",
      "trajectory saved as: cables_1_without_depth-False_pixel-False.npy\n",
      "#################### Results...\n",
      "{'rmse': 0.007509604244555598, 'mean': 0.007082647340240851, 'median': 0.00725747650105077, 'std': 0.0024960493912634498, 'min': 0.000818154093833898, 'max': 0.014809809787479363, 'sse': 0.0664323156618003}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t0.014810\n",
      "      mean\t0.007083\n",
      "    median\t0.007257\n",
      "       min\t0.000818\n",
      "      rmse\t0.007510\n",
      "       sse\t0.066432\n",
      "       std\t0.002496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/cables_1\"\n",
    "args.without_depth = False\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_path = save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traj_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"helo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/sfm_house_loop', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': False, 'pixelformer': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556 556\n",
      "initialising depth with 0\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "556it [00:56,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "556 556\n",
      "#################### Results...\n",
      "{'rmse': 0.9055769436950657, 'mean': 0.6987885443714965, 'median': 0.45451882392397175, 'std': 0.5759897318592245, 'min': 0.10234111085056768, 'max': 2.357951934089896, 'sse': 455.95869812936553}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t2.357952\n",
      "      mean\t0.698789\n",
      "    median\t0.454519\n",
      "       min\t0.102341\n",
      "      rmse\t0.905577\n",
      "       sse\t455.958698\n",
      "       std\t0.575990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/sfm_house_loop\"\n",
    "args.without_depth = False\n",
    "args.pixelformer = True\n",
    "\n",
    "traj_est = droid_track(pixelformer_depth_folder = 'pixelformer_depth')\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/sfm_house_loop', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [344, 560], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556 556\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "556it [00:58,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "556 556\n",
      "#################### Results...\n",
      "{'rmse': 1.8065393121814914, 'mean': 1.3517021817749861, 'median': 1.2135431081401358, 'std': 1.1985347296769997, 'min': 0.07059018017548883, 'max': 4.525181218202944, 'sse': 1814.55286327019}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t4.525181\n",
      "      mean\t1.351702\n",
      "    median\t1.213543\n",
      "       min\t0.070590\n",
      "      rmse\t1.806539\n",
      "       sse\t1814.552863\n",
      "       std\t1.198535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/sfm_house_loop\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/table_3_long', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [240, 320], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 1180\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [00:56, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "1180 1180\n",
      "trajectory saved as: table_3_long_without_depth-True_pixel-False.npy\n",
      "#################### Results...\n",
      "{'rmse': 0.00715659123219198, 'mean': 0.0063188272657054585, 'median': 0.0053520903358363105, 'std': 0.0033599434594743425, 'min': 0.0005384162464952738, 'max': 0.01658057304588858, 'sse': 0.04261237598981969}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t0.016581\n",
      "      mean\t0.006319\n",
      "    median\t0.005352\n",
      "       min\t0.000538\n",
      "      rmse\t0.007157\n",
      "       sse\t0.042612\n",
      "       std\t0.003360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/table_3_long\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/table_3', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [344, 560], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180 1180\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [00:51, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "1180 1180\n",
      "trajectory saved as: table_3_without_depth-True_pixel-False.npy\n",
      "#################### Results...\n",
      "{'rmse': 0.007029612340294209, 'mean': 0.006268652791011161, 'median': 0.005539875413187815, 'std': 0.003181107014950082, 'min': 0.000633829455938819, 'max': 0.015877415527288764, 'sse': 0.04116306956246225}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t0.015877\n",
      "      mean\t0.006269\n",
      "    median\t0.005540\n",
      "       min\t0.000634\n",
      "      rmse\t0.007030\n",
      "       sse\t0.041163\n",
      "       std\t0.003181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/table_3\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: {'datapath': '../datasets/ETH3D/train/table_3_long_fb', 'weights': '../droid.pth', 'buffer': 1024, 'image_size': [344, 560], 'disable_vis': True, 'beta': 0.5, 'filter_thresh': 2.0, 'warmup': 8, 'keyframe_thresh': 3.5, 'frontend_thresh': 16.0, 'frontend_window': 16, 'frontend_radius': 1, 'frontend_nms': 0, 'stereo': False, 'depth': False, 'backend_thresh': 22.0, 'backend_radius': 2, 'backend_nms': 3, 'upsample': False, 'reconstruction_path': '../saved_reconstruction', 'without_depth': True, 'pixelformer': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359 2359\n",
      "../droid.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2359it [01:51, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "################################\n",
      "Global BA Iteration #1\n",
      "Global BA Iteration #2\n",
      "Global BA Iteration #3\n",
      "Global BA Iteration #4\n",
      "Global BA Iteration #5\n",
      "Global BA Iteration #6\n",
      "Global BA Iteration #7\n",
      "Global BA Iteration #8\n",
      "Global BA Iteration #9\n",
      "Global BA Iteration #10\n",
      "Global BA Iteration #11\n",
      "Global BA Iteration #12\n",
      "2359 2359\n",
      "trajectory saved as: table_3_long_fb_without_depth-True_pixel-False.npy\n",
      "#################### Results...\n",
      "{'rmse': 0.0070611635888740325, 'mean': 0.006233421778473209, 'median': 0.0051789494077711514, 'std': 0.003317300734105412, 'min': 0.0008581114866220729, 'max': 0.016112952831418406, 'sse': 0.04153340601362406}\n",
      "APE w.r.t. translation part (m)\n",
      "(with Sim(3) Umeyama alignment)\n",
      "\n",
      "       max\t0.016113\n",
      "      mean\t0.006233\n",
      "    median\t0.005179\n",
      "       min\t0.000858\n",
      "      rmse\t0.007061\n",
      "       sse\t0.041533\n",
      "       std\t0.003317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# params to change\n",
    "args.datapath = \"../datasets/ETH3D/train/table_3_long_fb\"\n",
    "args.without_depth = True\n",
    "args.pixelformer = False\n",
    "\n",
    "traj_est = droid_track()\n",
    "save_traj(traj_est)\n",
    "evaluate_trajectory(traj_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBFB Experiment to check in long sequences.\n",
    "\n",
    "flowchart for FBFBFB experiment: \n",
    "\n",
    "1. make copy of seq folder : `cp -r table_3 table_3_long`\n",
    "\tand delete redundant files with command:\n",
    "``` bash\n",
    "cd table_3_long\n",
    "rm -rf extrinsics_1_2.txt calibration2.txt sequence_calibration.txt rgb2 depth.txt rgb.txt imu*\n",
    "```\n",
    "\n",
    "till now\n",
    "``` bash\n",
    "seq=cables_1\n",
    "cp -r $seq ${seq}_long\n",
    "cd ${seq}_long\n",
    "rm -rf extrinsics_1_2.txt calibration2.txt sequence_calibration.txt rgb2 depth.txt rgb.txt imu*\n",
    "\n",
    "```\n",
    "2. run cell [### removing redundant poses in gt.txt file.]\n",
    "3. replace gt file with new one \n",
    "(already inside ${seq}_long )\n",
    "``` bash \n",
    "rm groundtruth.txt\n",
    "mv groundtruth_long.txt groundtruth.txt\n",
    "\n",
    "```\n",
    "\n",
    "3. make multiple directories for fb, fbf, fbfb : \n",
    "``` bash \n",
    "cd ..\n",
    "cp -r ${seq}_long ${seq}_long_fb\n",
    "cp -r ${seq}_long ${seq}_long_fbf\n",
    "cp -r ${seq}_long ${seq}_long_fbfb\n",
    "\n",
    "```\n",
    "\n",
    "4. run cell for `fb`, then for `fbf`.....\n",
    "\n",
    "5. replace gt.txt and assoc.txt with new one and delete original one.\n",
    "``` bash \n",
    "seq=table_3\n",
    "rm ${seq}_long_fb/groundtruth.txt ${seq}_long_fb/associated.txt\n",
    "mv ${seq}_long_fb/groundtruth_long_fb.txt ${seq}_long_fb/groundtruth.txt\n",
    "mv ${seq}_long_fb/associated_long_fb.txt ${seq}_long_fb/associated.txt\n",
    "\n",
    "rm ${seq}_long_fbf/groundtruth.txt ${seq}_long_fbf/associated.txt\n",
    "mv ${seq}_long_fbf/groundtruth_long_fbf.txt ${seq}_long_fbf/groundtruth.txt\n",
    "mv ${seq}_long_fbf/associated_long_fbf.txt ${seq}_long_fbf/associated.txt\n",
    "\n",
    "rm ${seq}_long_fbfb/groundtruth.txt ${seq}_long_fbfb/associated.txt\n",
    "mv ${seq}_long_fbfb/groundtruth_long_fbfb.txt ${seq}_long_fbfb/groundtruth.txt\n",
    "mv ${seq}_long_fbfb/associated_long_fbfb.txt ${seq}_long_fbfb/associated.txt\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**misc**:\n",
    "To start for a seq from scatch(delete all)\n",
    "``` bash \n",
    "seq=cables_1\n",
    "rm -r ${seq}_long ${seq}_long_fb ${seq}_long_fbf ${seq}_long_fbfb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing redundant poses in gt.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total frames available:  98\n",
      "pose not available for timestamp: 11876.422479391\n",
      "poses_available = 97\n"
     ]
    }
   ],
   "source": [
    "seq=\"cables_2\"\n",
    "dir_path = f\"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/{seq}_long\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        #read all assoctiations timestamps first (we need only those tstamps)\n",
    "            \n",
    "        assoc_lines = assoc.readlines()\n",
    "        print(\"total frames available: \",len(assoc_lines))\n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "        \n",
    "        #now for every image in assoc.txt, find it's corresponding pose and add it to the gt.txt\n",
    "        # only if it is less than max_diff(=.01) apart in timestamps.\n",
    "        max_diff = .01\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long.txt\"),\"w\") as gt_long:\n",
    "            gt_long.write(gt_lines[0]) # adding commented first line\n",
    "            poses_available = 0\n",
    "            for line in assoc_lines:\n",
    "                line_list = line.strip().split(\" \")\n",
    "                #print(truncate(float(line_list[0]),2))\n",
    "                required_tstamp = float(line_list[0])\n",
    "                diff_list = np.abs(gt_tstamps-required_tstamp)\n",
    "                diffs = np.abs(gt_tstamps-required_tstamp)\n",
    "                index = int(np.argmin(diffs))\n",
    "                if diffs[index] <= max_diff:\n",
    "                    \"\"\"add pose for that image,depth pair in gt.txt\"\"\"\n",
    "                    closest_tstamp = gt_tstamps[index]\n",
    "                    #print(f\"closest tstamp to {required_tstamp} in gt is {closest_tstamp}\")\n",
    "                    poses_available += 1\n",
    "                    print_line = \" \".join(gt_dict[closest_tstamp])\n",
    "                    print_line = str(f\"{closest_tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                    gt_long.write(print_line)\n",
    "                    #print(print_line)\n",
    "                else:\n",
    "                    print(f\"pose not available for timestamp: {required_tstamp}\")\n",
    "                    gt_long.write(\"#not available\\n\")\n",
    "            print(\"poses_available =\",poses_available)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating long seqeuences manually by FB (forward-backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\" making gt.txt\"\"\"\n",
      "len(gt_lines) =  765\n",
      "last_ind =  763\n",
      "verify: n = 764, 2(n-1)+1 = 1527 , tstamps now = 1527\n",
      "written gt.txt !!\n",
      "\n",
      "\n",
      "\"\"\" making association.txt \"\"\"\n",
      "len(assoc_lines) =  976\n",
      "last_ind =  975\n",
      "verify: n = 976, 2(n-1)+1 = 1951 , tstamps now = 1951\n",
      "written assoc.txt for seq = sofa_1 !!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def truncate(num, n):\n",
    "    integer = int(num * (10**n))/(10**n)\n",
    "    return float(integer)\n",
    "seq = \"sofa_1\"\n",
    "dir_path = f\"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/{seq}_long_fb\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        \"\"\" making gt.txt\"\"\"\n",
    "        print('\"\"\"\" making gt.txt\"\"\"')\n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        print(\"len(gt_lines) = \",len(gt_lines))\n",
    "        #make gt dict\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(gt_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = gt_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        \n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long_fb.txt\"),\"w\") as gt_long:\n",
    "            for tstamp in gt_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(gt_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                gt_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written gt.txt !!\")\n",
    "\n",
    "        \n",
    "        \"\"\" making association.txt \"\"\"\n",
    "        print('\\n\\n\"\"\" making association.txt \"\"\"')\n",
    "        assoc_lines = assoc.readlines()\n",
    "        print(\"len(assoc_lines) = \",len(assoc_lines))\n",
    "        \n",
    "        #make assoc dict\n",
    "        assoc_dict = {}\n",
    "        for line in assoc_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            assoc_dict[float(line[0])] = line[1:]\n",
    "        assoc_tstamps = np.array(list(assoc_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = assoc_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            \n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            \n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind:.6f}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_3 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_4 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth10/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth10/{new_img_ind:.6f}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            os.system(cmd_3)\n",
    "            os.system(cmd_4)\n",
    "            \n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind:.6f}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind:.6f}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        \n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"associated_long_fb.txt\"),\"w\") as assoc_long:\n",
    "            for tstamp in assoc_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(assoc_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                assoc_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(f\"written assoc.txt for seq = {seq} !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating long seqeuences manually by FBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\" making gt.txt\"\"\"\n",
      "len(gt_lines) =  765\n",
      "last_ind =  763\n",
      "verify FB: n = 764, 2(n-1)+1 = 1527 , tstamps now = 1527\n",
      "fb_last_ind =  1526\n",
      "verify FBF: n = 764, 3(n-1)+1 = 2290 , tstamps now = 2290\n",
      "written gt.txt !!\n",
      "\n",
      "\n",
      "\"\"\" making association.txt \"\"\"\n",
      "len(assoc_lines) =  976\n",
      "last_ind =  975\n",
      "verify: n = 976, 2(n-1)+1 = 1951 , tstamps now = 1951\n",
      "fb_last_ind =  1950\n",
      "verify FBF: n = 976, 3(n-1)+1 = 2926 , tstamps now = 2926\n",
      "written assoc.txt for seq = sofa_1!!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def truncate(num, n):\n",
    "    integer = int(num * (10**n))/(10**n)\n",
    "    return float(integer)\n",
    "#seq = \"table_3\"\n",
    "dir_path = f\"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/{seq}_long_fbf\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        \"\"\" making gt.txt\"\"\"\n",
    "        print('\"\"\"\" making gt.txt\"\"\"')\n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        print(\"len(gt_lines) = \",len(gt_lines))\n",
    "        #make gt dict\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(gt_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = gt_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            \n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1 # original no. entries\n",
    "        print(f\"verify FB: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(gt_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= last_ind:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        \n",
    "        print(f\"verify FBF: n = {n}, 3(n-1)+1 = {3*n-2} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long_fbf.txt\"),\"w\") as gt_long:\n",
    "            for tstamp in gt_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(gt_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                gt_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written gt.txt !!\")\n",
    "        \n",
    "        \n",
    "        \"\"\" making association.txt \"\"\"\n",
    "        \n",
    "        print('\\n\\n\"\"\" making association.txt \"\"\"')\n",
    "        assoc_lines = assoc.readlines()\n",
    "        print(\"len(assoc_lines) = \",len(assoc_lines))\n",
    "        \n",
    "        #make assoc dict\n",
    "        assoc_dict = {}\n",
    "        for line in assoc_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            assoc_dict[float(line[0])] = line[1:]\n",
    "        assoc_tstamps = np.array(list(assoc_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = assoc_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            \n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind:.6f}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_3 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_4 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth10/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth10/{new_img_ind:.6f}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            os.system(cmd_3)\n",
    "            os.system(cmd_4)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind:.6f}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind:.6f}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= last_ind:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind:.6f}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_3 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_4 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth10/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth10/{new_img_ind:.6f}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            os.system(cmd_3)\n",
    "            os.system(cmd_4)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind:.6f}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind:.6f}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        print(f\"verify FBF: n = {n}, 3(n-1)+1 = {3*n-2} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"associated_long_fbf.txt\"),\"w\") as assoc_long:\n",
    "            for tstamp in assoc_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(assoc_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                assoc_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(f\"written assoc.txt for seq = {seq}!!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating long seqeuences manually by FBFB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\" making gt.txt\"\"\"\n",
      "len(gt_lines) =  765\n",
      "last_ind =  763\n",
      "verify FB: n = 764, 2(n-1)+1 = 1527 , tstamps now = 1527\n",
      "fb_last_ind =  1526\n",
      "verify FBF: n = 764, 4(n-1)+1 = 3053 , tstamps now = 3053\n",
      "written gt.txt !!\n",
      "\n",
      "\n",
      "\"\"\" making association.txt \"\"\"\n",
      "len(assoc_lines) =  976\n",
      "last_ind =  975\n",
      "verify: n = 976, 2(n-1)+1 = 1951 , tstamps now = 1951\n",
      "fb_last_ind =  1950\n",
      "verify FBF: n = 976, 4(n-1)+1 = 3901 , tstamps now = 3901\n",
      "written assoc.txt for seq = sofa_1 !!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def truncate(num, n):\n",
    "    integer = int(num * (10**n))/(10**n)\n",
    "    return float(integer)\n",
    "#seq=\"table_3\"\n",
    "dir_path = f\"/home/suraj/scratch/DROID-SLAM/datasets/ETH3D/train/{seq}_long_fbfb\"\n",
    "with open(os.path.join(dir_path,\"groundtruth.txt\")) as gt:\n",
    "    with open(os.path.join(dir_path,\"associated.txt\")) as assoc:\n",
    "        \"\"\" making gt.txt\"\"\"\n",
    "        print('\"\"\"\" making gt.txt\"\"\"')\n",
    "        # make a dictionary out of gt.txt\n",
    "        gt_lines = gt.readlines()\n",
    "        print(\"len(gt_lines) = \",len(gt_lines))\n",
    "        #make gt dict\n",
    "        gt_dict = {}\n",
    "        for line in gt_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            gt_dict[float(line[0])] = line[1:]\n",
    "        gt_tstamps = np.array(list(gt_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(gt_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = gt_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1 # original no. entries\n",
    "        print(f\"verify FB: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(gt_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = gt_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            gt_dict[curr_tstamp] = gt_dict[corresponding_tstamp]\n",
    "            gt_tstamps = np.append(gt_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        \n",
    "        print(f\"verify FBF: n = {n}, 4(n-1)+1 = {4*(n-1)+1} , tstamps now = {len(gt_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"groundtruth_long_fbfb.txt\"),\"w\") as gt_long:\n",
    "            for tstamp in gt_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(gt_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                gt_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(\"written gt.txt !!\")\n",
    "        \n",
    "        \n",
    "        \"\"\" making association.txt \"\"\"\n",
    "        \n",
    "        print('\\n\\n\"\"\" making association.txt \"\"\"')\n",
    "        assoc_lines = assoc.readlines()\n",
    "        print(\"len(assoc_lines) = \",len(assoc_lines))\n",
    "        \n",
    "        #make assoc dict\n",
    "        assoc_dict = {}\n",
    "        for line in assoc_lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\" \")\n",
    "            assoc_dict[float(line[0])] = line[1:]\n",
    "        assoc_tstamps = np.array(list(assoc_dict.keys()))\n",
    "        #print(gt_tstamps[0:10])\n",
    "\n",
    "        last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"last_ind = \",last_ind)\n",
    "        last_tstamp = assoc_tstamps[last_ind]\n",
    "        \n",
    "        curr_ind = last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = last_tstamp + (last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            \n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind:.6f}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_3 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_4 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth10/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth10/{new_img_ind:.6f}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            os.system(cmd_3)\n",
    "            os.system(cmd_4)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind:.6f}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind:.6f}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        n = last_ind + 1\n",
    "        print(f\"verify: n = {n}, 2(n-1)+1 = {2*n-1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        fb_last_ind = len(assoc_tstamps) - 1\n",
    "        print(\"fb_last_ind = \",fb_last_ind)\n",
    "        fb_last_tstamp = gt_tstamps[fb_last_ind]\n",
    "        curr_ind = fb_last_ind - 1\n",
    "        while curr_ind >= 0:\n",
    "            corresponding_tstamp = assoc_tstamps[curr_ind]\n",
    "            curr_tstamp = fb_last_tstamp + (fb_last_tstamp - corresponding_tstamp)\n",
    "            curr_tstamp = truncate(curr_tstamp,9)\n",
    "            assoc_dict[curr_tstamp] = copy.deepcopy(assoc_dict[corresponding_tstamp])\n",
    "            new_img_ind = truncate(curr_tstamp,6)\n",
    "            \n",
    "            #copy rgb and depth images\n",
    "            cmd_1 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][0]}\") + \" \" + os.path.join(dir_path,f\"rgb/{new_img_ind:.6f}.png\")\n",
    "            cmd_2 = \"cp \" + os.path.join(dir_path,f\"{assoc_dict[corresponding_tstamp][2]}\") + \" \" + os.path.join(dir_path,f\"depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_3 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth/{new_img_ind:.6f}.png\")\n",
    "            cmd_4 = \"cp \" + os.path.join(dir_path,f\"pixelformer_depth10/{truncate(float(assoc_dict[corresponding_tstamp][1]),6):.6f}.png\") + \" \" + os.path.join(dir_path,f\"pixelformer_depth10/{new_img_ind:.6f}.png\")\n",
    "            os.system(cmd_1)\n",
    "            os.system(cmd_2)\n",
    "            os.system(cmd_3)\n",
    "            os.system(cmd_4)\n",
    "            \n",
    "            assoc_dict[curr_tstamp][1] = f\"{curr_tstamp:.9f}\"\n",
    "            assoc_dict[curr_tstamp][0] = f\"rgb/{new_img_ind:.6f}.png\"\n",
    "            assoc_dict[curr_tstamp][2] = f\"depth/{new_img_ind:.6f}.png\"\n",
    "            assoc_tstamps = np.append(assoc_tstamps,curr_tstamp)\n",
    "            curr_ind -= 1\n",
    "        print(f\"verify FBF: n = {n}, 4(n-1)+1 = {4*(n-1)+1} , tstamps now = {len(assoc_tstamps)}\")\n",
    "        \n",
    "        #now for every entry of gt_dict write its entry in gt_long.txt\n",
    "        with open (os.path.join(dir_path,\"associated_long_fbfb.txt\"),\"w\") as assoc_long:\n",
    "            for tstamp in assoc_tstamps:\n",
    "                \n",
    "                print_line = \" \".join(assoc_dict[tstamp])\n",
    "                print_line = str(f\"{tstamp:.9f}\") + \" \" + print_line + \"\\n\"\n",
    "                assoc_long.write(print_line)\n",
    "                #print(print_line)\n",
    "        print(f\"written assoc.txt for seq = {seq} !!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
